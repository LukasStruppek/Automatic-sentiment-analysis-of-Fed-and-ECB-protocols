{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lb4653\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models, similarities \n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent folder of the documents\n",
    "parent_folder = \"\"\n",
    "# folder to read minutes plain text from\n",
    "text_folder = \"plain_text/\"\n",
    "# folder to save pre-processed minutes in\n",
    "pre_processed_folder = \"pre_processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lb4653\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading english: Package 'english' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lb4653\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lb4653\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('english')\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    text = text.lower()\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    #stemmer = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    for token in pos:\n",
    "        if re.search('[a-zA-Z]', token[0]):\n",
    "            if token[1].startswith('V'):\n",
    "                filtered_tokens.append(wordnet_lemmatizer.lemmatize(token[0], 'v'))\n",
    "            elif token[1].startswith('A'):\n",
    "                filtered_tokens.append(wordnet_lemmatizer.lemmatize(token[0], 'a'))\n",
    "            elif token[1].startswith('R'):\n",
    "                filtered_tokens.append(wordnet_lemmatizer.lemmatize(token[0], 's'))\n",
    "            else:\n",
    "                filtered_tokens.append(wordnet_lemmatizer.lemmatize(token[0], 'n'))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_most_common_words():\n",
    "    word_list = []\n",
    "    folders = os.listdir(parent_folder + pre_processed_folder)\n",
    "    for folder in folders:\n",
    "        files = os.listdir(parent_folder + pre_processed_folder + folder)\n",
    "        for file in files:\n",
    "            file_object = open(parent_folder + pre_processed_folder + folder + \"/\"+ file, \"r\", errors='ignore')\n",
    "            content = file_object.read()\n",
    "            stems = tokenize_and_stem(content)\n",
    "            words = remove_stopwords(stems)\n",
    "            for word in words:\n",
    "                word_list.append(word)\n",
    "    counts = Counter(word_list)\n",
    "    most_common_words = []\n",
    "    for word in counts.most_common(100):\n",
    "        most_common_words.append(word[0])\n",
    "    return most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = get_most_common_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "re_full_stop = re.compile('\\.+$')\n",
    "re_hyphen = re.compile('^-+.*|-+$')\n",
    "re_numbers = re.compile('.*[0-9]+.*')\n",
    "folders = os.listdir(parent_folder + pre_processed_folder)\n",
    "texts = []\n",
    "for folder in folders:\n",
    "    files = os.listdir(parent_folder + pre_processed_folder + folder)\n",
    "    for file in files:\n",
    "        date = datetime.datetime(int(file[:4]), int(file[4:6]), int(file[6:8]))\n",
    "        if date >= datetime.datetime(1990, 1, 1) and date <= datetime.datetime(2020, 12, 31):\n",
    "            word_list = []\n",
    "            file_object = open(parent_folder + pre_processed_folder + folder + \"/\"+ file, \"r\", errors='ignore')\n",
    "            content = file_object.read()\n",
    "            stems = tokenize_and_stem(content)\n",
    "            words = remove_stopwords(stems)\n",
    "            for word in words:\n",
    "                # Removing hyphens at the beginning and full stops at the end of words\n",
    "                word = re.sub(re_full_stop, '', word)\n",
    "                word = re.sub(re_hyphen, '', word)\n",
    "                # No words containing numbers\n",
    "                if not re.search(re_numbers, word):\n",
    "                    if len(word) > 2 and word not in most_common_words:\n",
    "                        word_list.append(word)\n",
    "            texts.append(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"export\" + 0.030*\"dollar\" + 0.028*\"u.s\" + 0.020*\"import\" + 0.015*\"value\" + 0.015*\"trade\" + 0.013*\"united\" + 0.013*\"currency\" + 0.012*\"country\" + 0.012*\"deficit\"'),\n",
       " (1,\n",
       "  '0.009*\"cost\" + 0.008*\"many\" + 0.007*\"contact\" + 0.007*\"current\" + 0.007*\"factor\" + 0.007*\"aggregate\" + 0.007*\"relatively\" + 0.006*\"productivity\" + 0.006*\"wage\" + 0.006*\"firm\"'),\n",
       " (2,\n",
       "  '0.014*\"agree\" + 0.012*\"objective\" + 0.011*\"target\" + 0.011*\"appropriate\" + 0.009*\"judge\" + 0.009*\"several\" + 0.008*\"downside\" + 0.007*\"saw\" + 0.007*\"asset\" + 0.007*\"toward\"'),\n",
       " (3,\n",
       "  '0.035*\"system\" + 0.028*\"open\" + 0.025*\"operation\" + 0.023*\"transaction\" + 0.022*\"account\" + 0.021*\"vote\" + 0.019*\"agency\" + 0.019*\"manager\" + 0.013*\"discussion\" + 0.013*\"facility\"'),\n",
       " (4,\n",
       "  '0.023*\"core\" + 0.022*\"forecast\" + 0.021*\"gdp\" + 0.017*\"pce\" + 0.015*\"compensation\" + 0.014*\"projection\" + 0.014*\"longer-run\" + 0.013*\"project\" + 0.013*\"end\" + 0.012*\"food\"'),\n",
       " (5,\n",
       "  '0.031*\"loan\" + 0.023*\"home\" + 0.021*\"mortgage\" + 0.021*\"household\" + 0.015*\"income\" + 0.014*\"commercial\" + 0.011*\"issuance\" + 0.011*\"start\" + 0.009*\"strong\" + 0.009*\"expand\"'),\n",
       " (6,\n",
       "  '0.018*\"manufacturing\" + 0.018*\"equipment\" + 0.013*\"vehicle\" + 0.013*\"output\" + 0.013*\"construction\" + 0.012*\"motor\" + 0.012*\"payroll\" + 0.011*\"capital\" + 0.011*\"government\" + 0.010*\"average\"'),\n",
       " (7,\n",
       "  '0.035*\"yield\" + 0.024*\"treasury\" + 0.022*\"bond\" + 0.020*\"spread\" + 0.018*\"equity\" + 0.015*\"investor\" + 0.014*\"index\" + 0.013*\"corporate\" + 0.013*\"fomc\" + 0.011*\"net\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passes: Number of passes through the entire corpus\n",
    "# chunksize: Number of documents to load into memory at a time and process E step of EM.\n",
    "# update_every: number of chunks to process prior to moving onto the M step of EM.\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.LdaModel(corpus, num_topics=8, \n",
    "                            id2word=dictionary, \n",
    "                            passes=200, \n",
    "                            chunksize=5000,\n",
    "                            update_every = 1)\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"loan\" + 0.025*\"yield\" + 0.023*\"treasury\" + 0.018*\"bond\" + 0.018*\"spread\" + 0.014*\"commercial\" + 0.014*\"issuance\" + 0.012*\"corporate\" + 0.012*\"equity\" + 0.011*\"net\"'),\n",
       " (1,\n",
       "  '0.018*\"equipment\" + 0.013*\"average\" + 0.012*\"construction\" + 0.012*\"capital\" + 0.011*\"payroll\" + 0.011*\"end\" + 0.010*\"worker\" + 0.010*\"nominal\" + 0.009*\"july\" + 0.009*\"october\"'),\n",
       " (2,\n",
       "  '0.049*\"system\" + 0.036*\"open\" + 0.033*\"operation\" + 0.030*\"transaction\" + 0.028*\"vote\" + 0.027*\"account\" + 0.025*\"manager\" + 0.017*\"facility\" + 0.014*\"discussion\" + 0.014*\"rrp\"'),\n",
       " (3,\n",
       "  '0.022*\"forecast\" + 0.021*\"projection\" + 0.019*\"longer-run\" + 0.019*\"gdp\" + 0.016*\"objective\" + 0.014*\"core\" + 0.014*\"run\" + 0.013*\"anticipate\" + 0.013*\"project\" + 0.011*\"medium\"'),\n",
       " (4,\n",
       "  '0.027*\"export\" + 0.027*\"dollar\" + 0.026*\"u.s\" + 0.017*\"import\" + 0.013*\"trade\" + 0.012*\"value\" + 0.012*\"currency\" + 0.012*\"united\" + 0.010*\"emerge\" + 0.010*\"country\"'),\n",
       " (5,\n",
       "  '0.017*\"target\" + 0.013*\"agree\" + 0.012*\"appropriate\" + 0.011*\"maintain\" + 0.010*\"asset\" + 0.010*\"objective\" + 0.010*\"statement\" + 0.010*\"toward\" + 0.009*\"stability\" + 0.008*\"future\"'),\n",
       " (6,\n",
       "  '0.024*\"home\" + 0.019*\"vehicle\" + 0.018*\"motor\" + 0.017*\"household\" + 0.016*\"output\" + 0.015*\"income\" + 0.015*\"manufacturing\" + 0.012*\"start\" + 0.011*\"mortgage\" + 0.010*\"construction\"'),\n",
       " (7,\n",
       "  '0.010*\"many\" + 0.009*\"number\" + 0.009*\"several\" + 0.008*\"cost\" + 0.008*\"factor\" + 0.007*\"potential\" + 0.007*\"concern\" + 0.007*\"contact\" + 0.007*\"uncertainty\" + 0.006*\"current\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.4)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.LdaModel(corpus, num_topics=8, \n",
    "                            id2word=dictionary, \n",
    "                            passes=200, \n",
    "                            chunksize=5000,\n",
    "                            update_every = 1)\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.039*\"loan\" + 0.018*\"bond\" + 0.017*\"commercial\" + 0.017*\"mortgage\" + 0.016*\"issuance\" + 0.015*\"spread\" + 0.011*\"corporate\" + 0.011*\"standard\" + 0.011*\"firm\" + 0.010*\"participation\"'),\n",
       " (1,\n",
       "  '0.024*\"projection\" + 0.024*\"forecast\" + 0.022*\"gdp\" + 0.021*\"longer-run\" + 0.015*\"objective\" + 0.014*\"run\" + 0.014*\"project\" + 0.012*\"anticipate\" + 0.011*\"since\" + 0.010*\"system\"'),\n",
       " (2,\n",
       "  '0.038*\"core\" + 0.027*\"index\" + 0.027*\"end\" + 0.025*\"food\" + 0.024*\"pce\" + 0.021*\"survey\" + 0.020*\"compensation\" + 0.018*\"rrp\" + 0.018*\"discussion\" + 0.017*\"york\"'),\n",
       " (3,\n",
       "  '0.009*\"many\" + 0.009*\"factor\" + 0.008*\"several\" + 0.008*\"number\" + 0.008*\"cost\" + 0.007*\"household\" + 0.007*\"contact\" + 0.007*\"high\" + 0.007*\"concern\" + 0.006*\"uncertainty\"'),\n",
       " (4,\n",
       "  '0.032*\"u.s\" + 0.030*\"export\" + 0.028*\"dollar\" + 0.021*\"import\" + 0.015*\"trade\" + 0.013*\"value\" + 0.013*\"united\" + 0.012*\"currency\" + 0.011*\"emerge\" + 0.011*\"deficit\"'),\n",
       " (5,\n",
       "  '0.014*\"agree\" + 0.013*\"target\" + 0.012*\"objective\" + 0.012*\"appropriate\" + 0.010*\"maintain\" + 0.009*\"toward\" + 0.009*\"stability\" + 0.008*\"statement\" + 0.008*\"consistent\" + 0.008*\"asset\"'),\n",
       " (6,\n",
       "  '0.024*\"treasury\" + 0.021*\"yield\" + 0.014*\"fomc\" + 0.014*\"operation\" + 0.012*\"system\" + 0.011*\"debt\" + 0.011*\"asset\" + 0.011*\"investor\" + 0.011*\"money\" + 0.011*\"open\"'),\n",
       " (7,\n",
       "  '0.015*\"home\" + 0.014*\"vehicle\" + 0.014*\"equipment\" + 0.014*\"construction\" + 0.014*\"motor\" + 0.013*\"manufacturing\" + 0.010*\"output\" + 0.010*\"july\" + 0.010*\"second\" + 0.010*\"fourth\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.6)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.LdaModel(corpus, num_topics=8, \n",
    "                            id2word=dictionary, \n",
    "                            passes=200, \n",
    "                            chunksize=5000,\n",
    "                            update_every = 1)\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"system\" + 0.024*\"open\" + 0.021*\"operation\" + 0.019*\"transaction\" + 0.018*\"account\" + 0.017*\"debt\" + 0.017*\"manager\" + 0.016*\"agency\" + 0.016*\"vote\" + 0.013*\"treasury\"'),\n",
       " (1,\n",
       "  '0.038*\"dollar\" + 0.027*\"u.s\" + 0.018*\"currency\" + 0.017*\"united\" + 0.016*\"emerge\" + 0.016*\"country\" + 0.014*\"japan\" + 0.013*\"euro\" + 0.013*\"export\" + 0.012*\"european\"'),\n",
       " (2,\n",
       "  '0.022*\"equipment\" + 0.022*\"manufacturing\" + 0.019*\"vehicle\" + 0.018*\"motor\" + 0.016*\"capital\" + 0.016*\"industrial\" + 0.016*\"output\" + 0.012*\"third\" + 0.012*\"order\" + 0.012*\"fourth\"'),\n",
       " (3,\n",
       "  '0.009*\"many\" + 0.009*\"household\" + 0.008*\"factor\" + 0.008*\"cost\" + 0.007*\"contact\" + 0.007*\"number\" + 0.007*\"several\" + 0.007*\"high\" + 0.007*\"relatively\" + 0.006*\"firm\"'),\n",
       " (4,\n",
       "  '0.012*\"agree\" + 0.011*\"target\" + 0.011*\"objective\" + 0.011*\"appropriate\" + 0.009*\"toward\" + 0.008*\"consistent\" + 0.007*\"stability\" + 0.007*\"judge\" + 0.007*\"assessment\" + 0.007*\"information\"'),\n",
       " (5,\n",
       "  '0.036*\"loan\" + 0.023*\"home\" + 0.020*\"mortgage\" + 0.015*\"commercial\" + 0.015*\"issuance\" + 0.013*\"start\" + 0.013*\"payroll\" + 0.011*\"government\" + 0.011*\"construction\" + 0.010*\"average\"'),\n",
       " (6,\n",
       "  '0.022*\"core\" + 0.019*\"survey\" + 0.018*\"pce\" + 0.015*\"compensation\" + 0.014*\"longer-run\" + 0.014*\"end\" + 0.013*\"food\" + 0.013*\"index\" + 0.011*\"information\" + 0.011*\"household\"'),\n",
       " (7,\n",
       "  '0.029*\"yield\" + 0.021*\"gdp\" + 0.019*\"forecast\" + 0.017*\"treasury\" + 0.016*\"bond\" + 0.016*\"equity\" + 0.015*\"import\" + 0.014*\"export\" + 0.013*\"index\" + 0.013*\"net\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.6)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.LdaModel(corpus, num_topics=8, \n",
    "                            id2word=dictionary, \n",
    "                            passes=200, \n",
    "                            chunksize=5000,\n",
    "                            update_every = 1)\n",
    "lda.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
